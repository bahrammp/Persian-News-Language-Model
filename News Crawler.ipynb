{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this news agency web site there is an \"archive\" page in which all published news are listed.in this page you can filter news by category and publish date.\n",
    "if you open the link bellow you can see the archive page:\n",
    "\n",
    "        http://www.entekhab.ir/fa/archive?service_id=18&sec_id=0&cat_id=0&rpp=100&from_date=1389/10/01&to_date=1397/01/30&p=1\n",
    "        \n",
    "there are 3 important keywords in the url :\n",
    "service_id : specifies category of news\n",
    "rpp : specifies number of news in each search page\n",
    "p : page number\n",
    "\n",
    "we use bs4 to parse web pages and urllib3 to send http requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we define a function to fetch an archive web page given page number and category of news(service_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entekhab_get_page_for_category(page_number, service_id):\n",
    "    ent_path = 'http://www.entekhab.ir/fa/archive?service_id={}&sec_id=0&cat_id=0&rpp=100&from_date=1389/10/01&to_date=1397/01/30&p={}'\n",
    "    return ent_path.format(service_id,page_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the function bellow parses the output of the previous function and returns a list of news link.this function takes page number and news category(service_id) as inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links_from_archive(page_number, service_id):\n",
    "    try:\n",
    "        http = urllib3.PoolManager()\n",
    "        r = http.request('GET', entekhab_get_page_for_category(str(page_number),service_id))\n",
    "        d = r.data.decode('utf-8')\n",
    "        soup = BeautifulSoup(d,'lxml')\n",
    "        r = soup.find_all('div',class_='news_archive_container')[0]\n",
    "        r = r.find_all('div',class_='linear_news')\n",
    "#         print(d)\n",
    "        links = list()\n",
    "        for i in r :\n",
    "#             print(i)\n",
    "    #         links.append('http://www.entekhab.ir/fa/news/'+i.find_all('a')[0]['href'].split('/')[3])\n",
    "            links.append({'service_id' : service_id, 'url' :'http://www.entekhab.ir/fa/news/'+i.find_all('a')[0]['href'].split('/')[3]})\n",
    "        return links\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"ERROR IN get_links_from_archive() page_number: \" + str(page_number))\n",
    "        return ['http://www.entekhab.ir']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, {'service_id': 2, 'url': 'http://www.entekhab.ir/fa/news/389088'})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll = get_links_from_archive(12,service_id=2)\n",
    "len(ll),ll[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the next thing to do is to fetch data of each page and store it in mongoDB.\n",
    "\"get_data_of_page\" does this job . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_of_page(page_url):\n",
    "    try:\n",
    "        http = urllib3.PoolManager()\n",
    "        r = http.request('GET', page_url['url'])\n",
    "        return {'url': page_url['url'],\n",
    "                'content':r.data,\n",
    "                'service_id':page_url['service_id']}\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return {'url': page_url['url']\n",
    "            ,'content':\"PAGE_DATA_NOT_FETCHED_CURRECTLY\",\n",
    "               'service_id':-1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"get_all_pages_data\" function takes page number and service_id as inputs and thread number as an optional input and returns data of each news link in the archive page with news category = service_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing.dummy import Pool as ThreadPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_pages_data(page_number,service_id,thread_number=2):\n",
    "    pool = ThreadPool(thread_number)\n",
    "    results = pool.map(get_data_of_page,\n",
    "                       get_links_from_archive(str(page_number),service_id=service_id))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the next thing is to run a for loop on the above function and store the output to mongoDB.\n",
    "bellow is a sample output of each documnet in mongoDB:\n",
    "\n",
    "        {'url': 'http://www.entekhab.ir/fa/news/389088',\n",
    "         'content':\"<html>...</html>\",\n",
    "         'service_id':2}\n",
    "         \n",
    "         \n",
    "we collect news in these four categories\n",
    "\n",
    "| category | service_id |\n",
    "|----------|------------|\n",
    "|politics|2|\n",
    "|economics|5|\n",
    "|havades|10|\n",
    "|art and culture|18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "client = MongoClient(\"mongodb://localhost:27017\")\n",
    "db = client.sentimententekhab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we change service_id and run this loop again to collect news in different categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserted batch 1\n",
      "inserted batch 2\n",
      "inserted batch 3\n",
      "inserted batch 4\n",
      "inserted batch 5\n",
      "inserted batch 6\n",
      "inserted batch 7\n",
      "inserted batch 8\n",
      "inserted batch 9\n",
      "inserted batch 10\n",
      "inserted batch 11\n",
      "inserted batch 12\n",
      "inserted batch 13\n",
      "inserted batch 14\n",
      "inserted batch 15\n",
      "inserted batch 16\n",
      "inserted batch 17\n",
      "inserted batch 18\n",
      "inserted batch 19\n",
      "inserted batch 20\n",
      "inserted batch 21\n",
      "inserted batch 22\n",
      "inserted batch 23\n",
      "inserted batch 24\n",
      "inserted batch 25\n",
      "inserted batch 26\n",
      "inserted batch 27\n",
      "inserted batch 28\n",
      "inserted batch 29\n",
      "inserted batch 30\n",
      "inserted batch 31\n",
      "inserted batch 32\n",
      "inserted batch 33\n",
      "inserted batch 34\n",
      "inserted batch 35\n",
      "inserted batch 36\n",
      "inserted batch 37\n",
      "inserted batch 38\n",
      "inserted batch 39\n",
      "inserted batch 40\n",
      "inserted batch 41\n",
      "inserted batch 42\n",
      "documents must be a non-empty list\n",
      "error in 43\n",
      "documents must be a non-empty list\n",
      "error in 44\n",
      "documents must be a non-empty list\n",
      "error in 45\n",
      "documents must be a non-empty list\n",
      "error in 46\n",
      "documents must be a non-empty list\n",
      "error in 47\n",
      "documents must be a non-empty list\n",
      "error in 48\n",
      "documents must be a non-empty list\n",
      "error in 49\n",
      "documents must be a non-empty list\n",
      "error in 50\n",
      "documents must be a non-empty list\n",
      "error in 51\n",
      "documents must be a non-empty list\n",
      "error in 52\n",
      "documents must be a non-empty list\n",
      "error in 53\n",
      "documents must be a non-empty list\n",
      "error in 54\n",
      "documents must be a non-empty list\n",
      "error in 55\n",
      "documents must be a non-empty list\n",
      "error in 56\n",
      "documents must be a non-empty list\n",
      "error in 57\n",
      "documents must be a non-empty list\n",
      "error in 58\n",
      "documents must be a non-empty list\n",
      "error in 59\n",
      "documents must be a non-empty list\n",
      "error in 60\n",
      "documents must be a non-empty list\n",
      "error in 61\n",
      "documents must be a non-empty list\n",
      "error in 62\n",
      "documents must be a non-empty list\n",
      "error in 63\n",
      "documents must be a non-empty list\n",
      "error in 64\n",
      "documents must be a non-empty list\n",
      "error in 65\n",
      "documents must be a non-empty list\n",
      "error in 66\n",
      "documents must be a non-empty list\n",
      "error in 67\n",
      "documents must be a non-empty list\n",
      "error in 68\n",
      "documents must be a non-empty list\n",
      "error in 69\n",
      "documents must be a non-empty list\n",
      "error in 70\n",
      "documents must be a non-empty list\n",
      "error in 71\n",
      "documents must be a non-empty list\n",
      "error in 72\n",
      "documents must be a non-empty list\n",
      "error in 73\n",
      "documents must be a non-empty list\n",
      "error in 74\n",
      "documents must be a non-empty list\n",
      "error in 75\n",
      "documents must be a non-empty list\n",
      "error in 76\n",
      "documents must be a non-empty list\n",
      "error in 77\n",
      "documents must be a non-empty list\n",
      "error in 78\n",
      "documents must be a non-empty list\n",
      "error in 79\n",
      "documents must be a non-empty list\n",
      "error in 80\n",
      "documents must be a non-empty list\n",
      "error in 81\n",
      "documents must be a non-empty list\n",
      "error in 82\n",
      "documents must be a non-empty list\n",
      "error in 83\n",
      "documents must be a non-empty list\n",
      "error in 84\n",
      "documents must be a non-empty list\n",
      "error in 85\n",
      "documents must be a non-empty list\n",
      "error in 86\n",
      "documents must be a non-empty list\n",
      "error in 87\n",
      "documents must be a non-empty list\n",
      "error in 88\n",
      "documents must be a non-empty list\n",
      "error in 89\n",
      "documents must be a non-empty list\n",
      "error in 90\n",
      "documents must be a non-empty list\n",
      "error in 91\n",
      "documents must be a non-empty list\n",
      "error in 92\n",
      "documents must be a non-empty list\n",
      "error in 93\n",
      "documents must be a non-empty list\n",
      "error in 94\n",
      "documents must be a non-empty list\n",
      "error in 95\n",
      "documents must be a non-empty list\n",
      "error in 96\n",
      "documents must be a non-empty list\n",
      "error in 97\n",
      "documents must be a non-empty list\n",
      "error in 98\n",
      "documents must be a non-empty list\n",
      "error in 99\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,40):\n",
    "    try:\n",
    "        e=get_all_pages_data(page_number=i,service_id=18,thread_number=12)\n",
    "        db.news.insert_many(e)\n",
    "        print('inserted batch {}'.format(i))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('error in '+str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the next thing is to parse html data and store news data and corresponding service_id\n",
    "so we define the news_parse function that takes html string as input and returns the text of news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def news_parser (news_text):\n",
    "    try:\n",
    "        soup = BeautifulSoup(news_text, \"lxml\")\n",
    "        ps = soup.find_all('div', {'class' : 'body col-xs-36'})[0]\n",
    "        ps= ps.find_all('p')\n",
    "        text=''\n",
    "        for p in ps :\n",
    "            text += p.get_text()\n",
    "            text += ' | ' # use this character instead of new line\n",
    "        \n",
    "        table = {}\n",
    "        table['text']=text\n",
    "    except Exception as e:\n",
    "        print('error in parsing : '+e)\n",
    "        table = {'text':'\\n'}\n",
    "    return table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we parse the data and grab text and service_id of each news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_dictionary = {}\n",
    "texts = list()\n",
    "labels = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n"
     ]
    }
   ],
   "source": [
    "cnt=0\n",
    "for i in db.news.find():\n",
    "    cnt+=1\n",
    "    if cnt % 1000 == 0:\n",
    "        print(cnt)\n",
    "    \n",
    "    if i['content'] == \"PAGE_DATA_NOT_FETCHED_CURRECTLY\":\n",
    "        print('error in db save ',i['url'])\n",
    "    else:\n",
    "        texts.append(new_parser(i['content'].decode('utf-8'))['text'])\n",
    "        labels.append(i['service_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the last step in data preparation is storing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_dictionary = {'text':texts, 'label':labels}\n",
    "df = pd.DataFrame(data=all_data_dictionary)\n",
    "df.to_pickle('news_out')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai_kernel",
   "language": "python",
   "name": "fastai_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
